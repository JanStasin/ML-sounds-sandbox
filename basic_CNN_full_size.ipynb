{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jansta/learn/acoustics'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "#from plot_audio import plot_specgram, plot_waveform\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mats = np.load('/Users/jansta/learn/acoustics/dict_mats.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = dict_mats['A']['can_opening'][2]\n",
    "\n",
    "\n",
    "# print(t.shape)\n",
    "# t2 = t / t.max()\n",
    "\n",
    "# print(np.max(t), np.min(t), np.mean(t), np.std(t))\n",
    "# #plt.plot(t)\n",
    "# plt.hist(t, bins=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'chirping_birds', 'vacuum_cleaner', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'clapping', 'fireworks', 'chainsaw', 'airplane', 'mouse_click', 'pouring_water', 'train', 'sheep', 'water_drops', 'church_bells', 'clock_alarm', 'keyboard_typing', 'wind', 'footsteps', 'frog', 'cow', 'brushing_teeth', 'car_horn', 'crackling_fire', 'helicopter', 'drinking_sipping', 'rain', 'insects', 'laughing', 'hen', 'engine', 'breathing', 'crying_baby', 'hand_saw', 'coughing', 'glass_breaking', 'snoring', 'toilet_flush', 'pig', 'washing_machine', 'clock_tick', 'sneezing', 'rooster', 'sea_waves', 'siren', 'cat', 'door_wood_creaks', 'crickets']\n"
     ]
    }
   ],
   "source": [
    "len(dict_mats['A']['can_opening'][3])\n",
    "\n",
    "all_labels = list(dict_mats['A'].keys())\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = all_labels[24:]\n",
    "\n",
    "encoded_labels = {}\n",
    "for i, label in enumerate(chosen_labels):\n",
    "    encoded_labels[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dict_mats, chosen_labels, encoded_labels, transform=None):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        for key in dict_mats.keys():\n",
    "            if key in chosen_labels:\n",
    "                for i in range(len(dict_mats[key])):\n",
    "                    self.X.append(dict_mats[key][i])\n",
    "                    self.y.append(encoded_labels[key])\n",
    "        \n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64,431)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with transform\n",
    "dataset = AudioDataset(dict_mats['A'], chosen_labels, encoded_labels, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Input batch size: torch.Size([4, 1, 64, 431])\n",
      "Labels: tensor([ 0, 15, 20, 15])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Input batch size: {inputs.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(\"-\" * 30)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to accomodate the full dataset. \n",
    "- Add more convolutional layers to capture more complex features.\n",
    "- Add batch normalization layers to stabilize and accelerate training.\n",
    "- Add dropout layers to prevent overfitting.\n",
    "- Use global average pooling before the fully connected layers to reduce the number of parameters.\n",
    "- Ensure the fully connected layers are appropriately sized for the increased complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(chosen_labels)\n",
    "\n",
    "# class AudioClassifNet(nn.Module):\n",
    "#     def __init__(self, n_classes) -> None:\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  \n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  \n",
    "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         #self.fc1 = nn.Linear(64 * 4 * 53, 256)  # Adjusted based on pooling layers\n",
    "#         self.fc1 = nn.Linear(64 * 4 * 26, 256)  # Adjusted based on pooling layers\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, n_classes)  # classes\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)  # out: (BS, 16, 64, 431)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)   # out: (BS, 16, 32, 215)\n",
    "#         x = self.conv2(x)  # out: (BS, 32, 32, 215)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)   # out: (BS, 32, 16, 107)\n",
    "#         x = self.conv3(x)  # out: (BS, 64, 8, 107)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)   # out: (BS, 64, 8, 53)\n",
    "#         x = self.pool(x)   # additional pooling layer - out: (BS, 64, 4, 26) \n",
    "#         x = self.flatten(x) # out: (4, 3328)\n",
    "#         x = self.fc1(x)  # out: (BS, 256)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)  # out: (BS, 128)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc3(x)  # out: (BS, n_classes)\n",
    "#         x = self.softmax(x)\n",
    "#         return x\n",
    "\n",
    "class AudioClassifNetBig(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        #  batch normalization layers to stabilize and accelerate training.\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # dropout layers to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 4 * 26, 512)  # Adjusted based on pooling layers\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # out: (BS, 32, 64, 431)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 32, 32, 215)\n",
    "        \n",
    "        x = self.conv2(x)  # out: (BS, 64, 32, 215)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 64, 16, 107)\n",
    "        \n",
    "        x = self.conv3(x)  # out: (BS, 128, 16, 107)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 128, 8, 53)\n",
    "        \n",
    "        x = self.conv4(x)  # out: (BS, 256, 8, 53)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 256, 4, 26)\n",
    "        \n",
    "        x = self.flatten(x) # out: (BS, 256 * 4 * 26)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)  # out: (BS, 512)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # out: (BS, 256)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)  # out: (BS, n_classes)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs found in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/8000, Loss: 3.323174\n",
      "Epoch 2/8000, Loss: 2.925733\n",
      "Epoch 4/8000, Loss: 2.776179\n",
      "Epoch 6/8000, Loss: 2.509570\n",
      "Epoch 8/8000, Loss: 2.341428\n",
      "Epoch 10/8000, Loss: 2.226864\n",
      "Epoch 12/8000, Loss: 2.004325\n",
      "Epoch 14/8000, Loss: 1.965495\n",
      "Epoch 16/8000, Loss: 1.832257\n",
      "Epoch 18/8000, Loss: 1.772507\n",
      "Epoch 20/8000, Loss: 1.713762\n",
      "Epoch 22/8000, Loss: 1.493820\n",
      "Epoch 24/8000, Loss: 1.436545\n",
      "Epoch 26/8000, Loss: 1.412231\n",
      "Epoch 28/8000, Loss: 1.397655\n",
      "Epoch 30/8000, Loss: 1.436562\n",
      "Epoch 32/8000, Loss: 1.239225\n",
      "Epoch 34/8000, Loss: 1.156875\n",
      "Epoch 36/8000, Loss: 1.256686\n",
      "Epoch 38/8000, Loss: 1.202768\n",
      "Epoch 40/8000, Loss: 1.100634\n",
      "Epoch 42/8000, Loss: 1.037949\n",
      "Epoch 44/8000, Loss: 1.099604\n",
      "Epoch 46/8000, Loss: 1.082543\n",
      "Epoch 48/8000, Loss: 0.901437\n",
      "Epoch 50/8000, Loss: 1.052319\n",
      "Epoch 52/8000, Loss: 1.019150\n",
      "Epoch 54/8000, Loss: 1.279825\n",
      "Epoch 56/8000, Loss: 0.891648\n",
      "Epoch 58/8000, Loss: 0.955948\n",
      "Epoch 60/8000, Loss: 1.004439\n",
      "Epoch 62/8000, Loss: 1.028184\n",
      "Epoch 64/8000, Loss: 0.881644\n",
      "Epoch 66/8000, Loss: 0.872958\n",
      "Epoch 68/8000, Loss: 0.967623\n",
      "Epoch 70/8000, Loss: 0.787864\n",
      "Epoch 72/8000, Loss: 1.001920\n",
      "Epoch 74/8000, Loss: 0.791691\n",
      "Epoch 76/8000, Loss: 0.774000\n"
     ]
    }
   ],
   "source": [
    "# Assuming the model is defined as AudioClassifNet\n",
    "model = AudioClassifNetBig(n_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "losses_epoch_mean = []\n",
    "NUM_EPOCHS = 8000\n",
    "CLIP = 1.0  # Gradient clipping value\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    losses_epoch = []\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            print(f\"NaN input at epoch {epoch}, batch {i}\")\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss at epoch {epoch}, batch {i}\")\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses_epoch.append(loss.item())\n",
    "    \n",
    "    losses_epoch_mean.append(np.mean(losses_epoch))\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'Epoch {epoch}/{NUM_EPOCHS}, Loss: {np.mean(losses_epoch):.6f}')\n",
    "\n",
    "sns.lineplot(x=list(range(len(losses_epoch_mean))), y=losses_epoch_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "y_val_hat = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    with torch.no_grad():\n",
    "        y_val_hat_temp = model(inputs).round()\n",
    "    \n",
    "    y_val.extend(y_val_temp.numpy())\n",
    "    y_val_hat.extend(y_val_hat_temp.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_val, np.argmax(y_val_hat, axis=1))\n",
    "print(f'Accuracy: {acc*100:.2f} %')\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_val, np.argmax(y_val_hat, axis=1))\n",
    "sns.heatmap(cm, annot=True, xticklabels=chosen_labels, yticklabels=chosen_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
