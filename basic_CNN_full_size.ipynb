{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jansta/learn/acoustics'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "#from plot_audio import plot_specgram, plot_waveform\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mats = np.load('/Users/jansta/learn/acoustics/dict_mats.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = dict_mats['A']['can_opening'][2]\n",
    "\n",
    "\n",
    "# print(t.shape)\n",
    "# t2 = t / t.max()\n",
    "\n",
    "# print(np.max(t), np.min(t), np.mean(t), np.std(t))\n",
    "# #plt.plot(t)\n",
    "# plt.hist(t, bins=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'chirping_birds', 'vacuum_cleaner', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'clapping', 'fireworks', 'chainsaw', 'airplane', 'mouse_click', 'pouring_water', 'train', 'sheep', 'water_drops', 'church_bells', 'clock_alarm', 'keyboard_typing', 'wind', 'footsteps', 'frog', 'cow', 'brushing_teeth', 'car_horn', 'crackling_fire', 'helicopter', 'drinking_sipping', 'rain', 'insects', 'laughing', 'hen', 'engine', 'breathing', 'crying_baby', 'hand_saw', 'coughing', 'glass_breaking', 'snoring', 'toilet_flush', 'pig', 'washing_machine', 'clock_tick', 'sneezing', 'rooster', 'sea_waves', 'siren', 'cat', 'door_wood_creaks', 'crickets']\n"
     ]
    }
   ],
   "source": [
    "len(dict_mats['A']['can_opening'][3])\n",
    "\n",
    "all_labels = list(dict_mats['A'].keys())\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = all_labels[24:]\n",
    "\n",
    "encoded_labels = {}\n",
    "for i, label in enumerate(chosen_labels):\n",
    "    encoded_labels[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dict_mats, chosen_labels, encoded_labels, transform=None):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        for key in dict_mats.keys():\n",
    "            if key in chosen_labels:\n",
    "                for i in range(len(dict_mats[key])):\n",
    "                    self.X.append(dict_mats[key][i])\n",
    "                    self.y.append(encoded_labels[key])\n",
    "        \n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64,431)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with transform\n",
    "dataset = AudioDataset(dict_mats['A'], chosen_labels, encoded_labels, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Input batch size: torch.Size([4, 1, 64, 431])\n",
      "Labels: tensor([ 0, 21,  4,  1])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Input batch size: {inputs.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(\"-\" * 30)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to accomodate the full dataset. \n",
    "- Add more convolutional layers to capture more complex features.\n",
    "- Add batch normalization layers to stabilize and accelerate training.\n",
    "- Add dropout layers to prevent overfitting.\n",
    "- Use global average pooling before the fully connected layers to reduce the number of parameters.\n",
    "- Ensure the fully connected layers are appropriately sized for the increased complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(chosen_labels)\n",
    "\n",
    "class AudioClassifNetBig(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        #  batch normalization layers to stabilize and accelerate training.\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # dropout layers to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 4 * 26, 512)  # Adjusted based on pooling layers\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # out: (BS, 32, 64, 431)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 32, 32, 215)\n",
    "        \n",
    "        x = self.conv2(x)  # out: (BS, 64, 32, 215)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 64, 16, 107)\n",
    "        \n",
    "        x = self.conv3(x)  # out: (BS, 128, 16, 107)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 128, 8, 53)\n",
    "        \n",
    "        x = self.conv4(x)  # out: (BS, 256, 8, 53)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 256, 4, 26)\n",
    "        \n",
    "        x = self.flatten(x) # out: (BS, 256 * 4 * 26)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)  # out: (BS, 512)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # out: (BS, 256)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)  # out: (BS, n_classes)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = AudioClassifNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs found in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an  instance of the model:\n",
    "model = AudioClassifNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/8000, Loss: 3.632030\n",
      "Epoch 100/8000, Loss: 0.471074\n",
      "Epoch 200/8000, Loss: 0.398045\n",
      "Epoch 300/8000, Loss: 0.493930\n",
      "Epoch 400/8000, Loss: 0.236736\n",
      "Epoch 500/8000, Loss: 0.494331\n",
      "Epoch 600/8000, Loss: 0.229933\n",
      "Epoch 700/8000, Loss: 0.397229\n",
      "Epoch 800/8000, Loss: 0.471671\n",
      "Epoch 900/8000, Loss: 0.092682\n",
      "Epoch 1000/8000, Loss: 0.334093\n",
      "Epoch 1100/8000, Loss: 0.257831\n",
      "Epoch 1200/8000, Loss: 0.413166\n",
      "Epoch 1300/8000, Loss: 0.088354\n",
      "Epoch 1400/8000, Loss: 0.321582\n",
      "Epoch 1500/8000, Loss: 0.051388\n",
      "Epoch 1600/8000, Loss: 0.243373\n",
      "Epoch 1700/8000, Loss: 0.159811\n",
      "Epoch 1800/8000, Loss: 0.074493\n",
      "Epoch 1900/8000, Loss: 0.030372\n"
     ]
    }
   ],
   "source": [
    "# Assuming the model is defined as AudioClassifNet\n",
    "model = AudioClassifNetBig(n_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "losses_epoch_mean = []\n",
    "NUM_EPOCHS = 8000\n",
    "CLIP = 1.0  # Gradient clipping value\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    losses_epoch = []\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Check for NaN in inputs\n",
    "        if torch.isnan(inputs).any():\n",
    "            print(f\"NaN input at epoch {epoch}, batch {i}\")\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss at epoch {epoch}, batch {i}\")\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses_epoch.append(loss.item())\n",
    "    \n",
    "    losses_epoch_mean.append(np.mean(losses_epoch))\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}/{NUM_EPOCHS}, Loss: {np.mean(losses_epoch):.6f}')\n",
    "\n",
    "sns.lineplot(x=list(range(len(losses_epoch_mean))), y=losses_epoch_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "y_val_hat = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    with torch.no_grad():\n",
    "        y_val_hat_temp = model(inputs).round()\n",
    "    \n",
    "    y_val.extend(y_val_temp.numpy())\n",
    "    y_val_hat.extend(y_val_hat_temp.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_val, np.argmax(y_val_hat, axis=1))\n",
    "print(f'Accuracy: {acc*100:.2f} %')\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_val, np.argmax(y_val_hat, axis=1))\n",
    "sns.heatmap(cm, annot=True, xticklabels=chosen_labels, yticklabels=chosen_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
