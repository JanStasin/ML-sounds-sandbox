{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "#from plot_audio import plot_specgram, plot_waveform\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mats = np.load('/Users/jansta/learn/acoustics/dict_mats.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = dict_mats['A']['can_opening'][2]\n",
    "\n",
    "\n",
    "# print(t.shape)\n",
    "# t2 = t / t.max()\n",
    "\n",
    "# print(np.max(t), np.min(t), np.mean(t), np.std(t))\n",
    "# #plt.plot(t)\n",
    "# plt.hist(t, bins=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_mats['A']['can_opening'][3])\n",
    "\n",
    "all_labels = dict_mats['A'].keys()\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = ['crickets', 'can_opening', 'chirping_birds', 'dog', 'chainsaw'][:4]\n",
    "encoded_labels = {'crickets': 0, 'can_opening': 1, 'chirping_birds': 2, 'dog': 3, 'chainsaw': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = list(all_labels)[20:26]\n",
    "\n",
    "encoded_labels = {}\n",
    "for i, label in enumerate(chosen_labels):\n",
    "    encoded_labels[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dict_mats, chosen_labels, encoded_labels, transform=None):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        for key in dict_mats.keys():\n",
    "            if key in chosen_labels:\n",
    "                for i in range(len(dict_mats[key])):\n",
    "                    self.X.append(dict_mats[key][i])\n",
    "                    self.y.append(encoded_labels[key])\n",
    "        \n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64,431)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with transform\n",
    "dataset = AudioDataset(dict_mats['A'], chosen_labels, encoded_labels, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataloader\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Input batch size: {inputs.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(\"-\" * 30)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(chosen_labels)\n",
    "\n",
    "class AudioClassifNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        #self.fc1 = nn.Linear(64 * 4 * 53, 256)  # Adjusted based on pooling layers\n",
    "        self.fc1 = nn.Linear(64 * 4 * 26, 256)  # Adjusted based on pooling layers\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, n_classes)  # classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # out: (BS, 16, 64, 431)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 16, 32, 215)\n",
    "        x = self.conv2(x)  # out: (BS, 32, 32, 215)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 32, 16, 107)\n",
    "        x = self.conv3(x)  # out: (BS, 64, 8, 107)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)   # out: (BS, 64, 8, 53)\n",
    "        x = self.pool(x)   # additional pooling layer - out: (BS, 64, 4, 26) \n",
    "        x = self.flatten(x) # out: (4, 3328)\n",
    "        x = self.fc1(x)  # out: (BS, 256)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)  # out: (BS, 128)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)  # out: (BS, n_classes)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs found in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an  instance of the model:\n",
    "model = AudioClassifNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# %% TRAINING\n",
    "losses_epoch_mean = []\n",
    "NUM_EPOCHS = 200\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    losses_epoch = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(inputs).any():\n",
    "            print(f\"NaN input at epoch {epoch}, batch {i}\")\n",
    "\n",
    "            i_err = inputs\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss at epoch {epoch}, batch {i}\")\n",
    "            l_err = loss\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses_epoch.append(loss.item())\n",
    "    \n",
    "    # if torch.isnan(loss):\n",
    "    #     break\n",
    "    \n",
    "    losses_epoch_mean.append(np.mean(losses_epoch))\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{NUM_EPOCHS}, Loss: {np.mean(losses_epoch):.6f}')\n",
    "\n",
    "sns.lineplot(x=list(range(len(losses_epoch_mean))), y=losses_epoch_mean)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "y_val_hat = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    with torch.no_grad():\n",
    "        y_val_hat_temp = model(inputs).round()\n",
    "    \n",
    "    y_val.extend(y_val_temp.numpy())\n",
    "    y_val_hat.extend(y_val_hat_temp.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_val, np.argmax(y_val_hat, axis=1))\n",
    "print(f'Accuracy: {acc*100:.2f} %')\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_val, np.argmax(y_val_hat, axis=1))\n",
    "sns.heatmap(cm, annot=True, xticklabels=chosen_labels, yticklabels=chosen_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
