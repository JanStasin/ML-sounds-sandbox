{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import random\n",
    "#from plot_audio import plot_specgram, plot_waveform\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mats = np.load('/Users/jansta/learn/acoustics/dict_mats_dB.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_mats['A']['can_opening'][3])\n",
    "\n",
    "all_labels = dict_mats['A'].keys()\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = ['crickets', 'can_opening', 'chirping_birds', 'dog', 'chainsaw'][:4]\n",
    "encoded_labels = {'crickets': 0, 'can_opening': 1, 'chirping_birds': 2, 'dog': 3, 'chainsaw': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = list(all_labels)[:20]\n",
    "print(chosen_labels)\n",
    "encoded_labels = {}\n",
    "for i, label in enumerate(chosen_labels):\n",
    "    encoded_labels[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dict_mats, chosen_labels, encoded_labels, transform=None):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        for key in dict_mats.keys():\n",
    "            if key in chosen_labels:\n",
    "                for i in range(len(dict_mats[key])):\n",
    "                    self.X.append(dict_mats[key][i])\n",
    "                    self.y.append(encoded_labels[key])\n",
    "        \n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64,431)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with transform\n",
    "dataset = AudioDataset(dict_mats['A'], chosen_labels, encoded_labels, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataloader\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Input batch size: {inputs.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(\"-\" * 30)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifNetXAI(nn.Module):\n",
    "    def __init__(self, n_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # First Convolutional Block\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            # First convolution: increase number of channels to 16\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # reduces height and width by 2\n",
    "            \n",
    "            # Second convolution: further increase channels to 32, also add BatchNorm\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Second Convolutional Block with a fixed channel progression\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Global average pooling to collapse the spatial dimensions to 1x1.\n",
    "        # This avoids having to hard-code the flattened size.\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected block, now starting from a known feature size (128)\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),              # Flattens (B, 128, 1, 1) into (B, 128)\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, store_feature_maps: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "          - Applies two convolutional blocks\n",
    "          - Optionally stores the feature maps (used for techniques such as Grad-CAM)\n",
    "          - Applies global average pooling to reduce the feature maps to a fixed size\n",
    "          - Propagates through the fully connected block\n",
    "      \n",
    "        Args:\n",
    "          x (torch.Tensor): Input tensor of shape [batch_size, 1, 64, 431]\n",
    "          store_feature_maps (bool, optional): If True, saves the output of conv_block2 \n",
    "                                                 for visualization. Defaults to False.\n",
    "      \n",
    "        Returns:\n",
    "          torch.Tensor: Output logits of shape [batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        \n",
    "        if store_feature_maps:\n",
    "            # Detaching feature maps for visualization (e.g., Grad-CAM)\n",
    "            self.feature_maps = x.detach()\n",
    "        \n",
    "        # Global average pooling: converts (B, 128, H, W) to (B, 128, 1, 1)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.fc_block(x)\n",
    "        # Note: Do not apply an activation like softmax here if you're using CrossEntropyLoss\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs found in {name}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an  instance of the model:\n",
    "model = AudioClassifNetXAI(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# %% TRAINING\n",
    "losses_epoch_mean = []\n",
    "NUM_EPOCHS = 500\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    losses_epoch = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(inputs).any():\n",
    "            print(f\"NaN input at epoch {epoch}, batch {i}\")\n",
    "            i_err = inputs\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses_epoch.append(loss.item())\n",
    "    \n",
    "    losses_epoch_mean.append(np.mean(losses_epoch))\n",
    "    if epoch % int(NUM_EPOCHS/10) == 0:\n",
    "        print(f'Epoch {epoch}/{NUM_EPOCHS}, Loss: {np.mean(losses_epoch):.12f}')\n",
    "\n",
    "sns.lineplot(x=list(range(len(losses_epoch_mean))), y=losses_epoch_mean)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "y_val_hat = []\n",
    "model.eval()\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    with torch.no_grad():\n",
    "        y_val_hat_temp = model(inputs)\n",
    "    \n",
    "    y_val.extend(y_val_temp.cpu().numpy())\n",
    "    y_val_hat.extend(y_val_hat_temp.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_val, np.argmax(y_val_hat, axis=1))\n",
    "print(f'Accuracy: {acc*100:.2f} %')\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_val, np.argmax(y_val_hat, axis=1))\n",
    "sns.heatmap(cm, annot=True, xticklabels=chosen_labels, yticklabels=chosen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradCAM import gradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"path_to_weights.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Select the target convolutional layer.\n",
    "target_layer = model.conv_block2[-1]\n",
    "\n",
    "# Create an instance of GradCAM with your model and target layer.\n",
    "grad_cam = GradCAM(model, target_layer)\n",
    "\n",
    "# Create a dummy input corresponding to one spectrogram [batch, channel, height, width]\n",
    "\n",
    "test_inp, _ = val_loader.dataset[0]  # Assuming val_loader.dataset[0] returns a tuple (input, target)\n",
    "test_inp = test_inp.unsqueeze(0)  # Add batch dimension\n",
    "#test_inp.requires_grad_(True) \n",
    "test_inp.size()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tensor = torch.randn(1, 1, 64, 431)\n",
    "\n",
    "# Generate the Grad-CAM heatmap:\n",
    "# Pass the input and (optionally) specify a target_class; otherwise the predicted class is used.\n",
    "cam_heatmap, pred_class = grad_cam.generate_cam(test_inp, target_class=None)\n",
    "predicted_label = list(encoded_labels.keys())[list(encoded_labels.values()).index(pred_class)]\n",
    "print(f\"Predicted class: {predicted_label}\")\n",
    "# Convert the heatmap to numpy and visualize it using matplotlib\n",
    "heatmap = cam_heatmap.squeeze().cpu().numpy()  # shape becomes (64, 431)\n",
    "plt.imshow(heatmap, cmap='jet', interpolation='bilinear')\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# When done, remove the hooks to avoid potential memory leaks.\n",
    "grad_cam.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = {}\n",
    "samples = {}\n",
    "model.eval()\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    #print(inputs.shape, y_val_temp.shape)\n",
    "    for j in range(inputs.shape[0]):\n",
    "        target_layer = model.conv_block2[-1]\n",
    "        grad_cam = gradCAM(model, target_layer)\n",
    "        single_input = inputs[j].unsqueeze(0)\n",
    "        cam_hm, pred_class = grad_cam.generate_cam(single_input, target_class=None)\n",
    "        predicted_label = list(encoded_labels.keys())[list(encoded_labels.values()).index(pred_class)]\n",
    "        print(f\"Predicted class: {predicted_label}\")\n",
    "        if predicted_label not in cams.keys():\n",
    "            cams[predicted_label] = [cam_hm]\n",
    "        else:\n",
    "            cams[predicted_label].append(cam_hm)\n",
    "\n",
    "        if predicted_label not in samples.keys():\n",
    "            samples[predicted_label] = inputs[j]\n",
    "\n",
    "\n",
    "class_cams = {}\n",
    "for key in cams.keys():\n",
    "    mean_class_cam = np.mean(cams[key], axis=0)\n",
    "    #print(mean_class_cam.shape)\n",
    "    class_cams[key] = mean_class_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for key in class_cams.keys():\n",
    "    plt.figure(figsize=(5, 10))\n",
    "    plt.imshow(class_cams[key][0,0,:,:], cmap='jet')\n",
    "    plt.title(f\"Class Activation Map for class: {key}\")\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.savefig(f'cam_{key}.png')\n",
    "\n",
    "# if save_output:\n",
    "#     np.save('class_cams.npy', class_cams)\n",
    "\n",
    "    # for key in class_cams.keys():\n",
    "    #     plt.figure(figsize=(5, 10))\n",
    "    #     plt.imshow(class_cams[key:[0,0,:,:], cmap='jet')\n",
    "    #     plt.title(f\"Class Activation Map for class: {key}\")\n",
    "    #     #plt.colorbar()\n",
    "    #     plt.show()\n",
    "    #     plt.savefig(f'cam_{key}.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
