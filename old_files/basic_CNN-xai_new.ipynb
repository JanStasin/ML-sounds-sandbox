{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jansta/learn/acoustics'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "#from plot_audio import plot_specgram, plot_waveform\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mats = np.load('/Users/jansta/learn/acoustics/dict_mats_dB.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dog', 'chirping_birds', 'vacuum_cleaner', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'clapping', 'fireworks', 'chainsaw', 'airplane', 'mouse_click', 'pouring_water', 'train', 'sheep', 'water_drops', 'church_bells', 'clock_alarm', 'keyboard_typing', 'wind', 'footsteps', 'frog', 'cow', 'brushing_teeth', 'car_horn', 'crackling_fire', 'helicopter', 'drinking_sipping', 'rain', 'insects', 'laughing', 'hen', 'engine', 'breathing', 'crying_baby', 'hand_saw', 'coughing', 'glass_breaking', 'snoring', 'toilet_flush', 'pig', 'washing_machine', 'clock_tick', 'sneezing', 'rooster', 'sea_waves', 'siren', 'cat', 'door_wood_creaks', 'crickets'])\n"
     ]
    }
   ],
   "source": [
    "len(dict_mats['A']['can_opening'][3])\n",
    "\n",
    "all_labels = dict_mats['A'].keys()\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = ['crickets', 'can_opening', 'chirping_birds', 'dog', 'chainsaw'][:4]\n",
    "encoded_labels = {'crickets': 0, 'can_opening': 1, 'chirping_birds': 2, 'dog': 3, 'chainsaw': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'chirping_birds', 'vacuum_cleaner', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'clapping', 'fireworks', 'chainsaw', 'airplane', 'mouse_click', 'pouring_water', 'train', 'sheep', 'water_drops', 'church_bells', 'clock_alarm', 'keyboard_typing', 'wind']\n"
     ]
    }
   ],
   "source": [
    "chosen_labels = list(all_labels)[:20]\n",
    "print(chosen_labels)\n",
    "encoded_labels = {}\n",
    "for i, label in enumerate(chosen_labels):\n",
    "    encoded_labels[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dict_mats, chosen_labels, encoded_labels, transform=None):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        for key in dict_mats.keys():\n",
    "            if key in chosen_labels:\n",
    "                for i in range(len(dict_mats[key])):\n",
    "                    self.X.append(dict_mats[key][i])\n",
    "                    self.y.append(encoded_labels[key])\n",
    "        \n",
    "        self.X = np.array(self.X)\n",
    "        self.y = np.array(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Add a channel dimension\n",
    "        sample = np.expand_dims(sample, axis=0)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        sample = torch.FloatTensor(sample)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64,431)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with transform\n",
    "dataset = AudioDataset(dict_mats['A'], chosen_labels, encoded_labels, transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Input batch size: torch.Size([4, 1, 64, 431])\n",
      "Labels: tensor([13, 16,  2, 10])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Input batch size: {inputs.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(\"-\" * 30)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(chosen_labels)\n",
    "\n",
    "class AudioClassifNetCAM(nn.Module):\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # First block: 2 convolutional layers + pooling\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Second block: 3 convolutional layers + global pooling\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Store feature maps after second convolutional block\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        \n",
    "        # Store feature maps for CAM visualization\n",
    "        self.feature_maps = x.detach()\n",
    "        \n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "    def getCAM(self, class_idx):\n",
    "        if self.feature_maps is None:\n",
    "            raise ValueError(\"Feature maps are not set. Run a forward pass first.\")\n",
    "        \n",
    "        if class_idx >= n_classes:\n",
    "            raise ValueError(f\"Class index {class_idx} is out of bounds for {n_classes} classes.\")\n",
    "        \n",
    "        # Get the feature maps from the last convolutional layer\n",
    "        feature_maps = self.feature_maps.squeeze(0)\n",
    "        \n",
    "        # Get the weights for the final fully connected layer\n",
    "        weights = self.fc_block[7].weight[class_idx].detach()\n",
    "        #print(f\"Weights shape before reshape: {weights.shape}\")\n",
    "        weights = weights.view(1,-1, 1, 1)\n",
    "        #print(f\"Weights shape after reshape: {weights.shape}\")\n",
    "    \n",
    "        cam = torch.sum(feature_maps * weights, dim=1, keepdim=True)\n",
    "        #print(f\"CAM shape after computation: {cam.shape}\")\n",
    "        \n",
    "        # Normalize the CAM\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / cam.max()\n",
    "        \n",
    "        return cam                 \n",
    "\n",
    "    def generate_cam_visualization(self, cam, test_inp):\n",
    "        \"\"\"\n",
    "        Generate CAM visualization for a given input\n",
    "        Args:\n",
    "            input_tensor (Tensor): Input audio spectrogram tensor\n",
    "            class_idx (int): Index of the target class   \n",
    "        Returns:\n",
    "            visualization (ndarray): CAM visualization overlaid on input\n",
    "        \"\"\"\n",
    "        #cam = self.getCAM(class_idx)\n",
    "\n",
    "        # First squeeze to remove singleton dimensions\n",
    "        cam = cam.squeeze(0)  # Remove batch dimension\n",
    "        cam = cam.squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        # Now resize to match input dimensions\n",
    "        # Get the height and width from input tensor\n",
    "        height, width = test_inp.shape[1:]\n",
    "\n",
    "        # Resize using both dimensions\n",
    "        cam = F.interpolate(\n",
    "            cam.unsqueeze(0).unsqueeze(0),  # Add back dimensions for interpolation\n",
    "            size=(height, width),           # Use both dimensions\n",
    "            mode='bilinear',\n",
    "            align_corners=True\n",
    "        )\n",
    "\n",
    "        # Remove added dimensions and convert to numpy\n",
    "        visualization = cam.squeeze().cpu().numpy()\n",
    "        \n",
    "        return visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs found in {name}\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an  instance of the model:\n",
    "model = AudioClassifNetCAM(n_classes)\n",
    "#from helper_functions import capture_gradients, resize_cam_to_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200, Loss: 2.990327878077\n",
      "Epoch 20/200, Loss: 1.230514698830\n",
      "Epoch 40/200, Loss: 1.007270149125\n",
      "Epoch 60/200, Loss: 0.227148406229\n",
      "Epoch 80/200, Loss: 0.260315243944\n",
      "Epoch 100/200, Loss: 0.425427594266\n",
      "Epoch 120/200, Loss: 0.050084567831\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# %% TRAINING\n",
    "losses_epoch_mean = []\n",
    "NUM_EPOCHS = 200\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    losses_epoch = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(inputs).any():\n",
    "            print(f\"NaN input at epoch {epoch}, batch {i}\")\n",
    "            i_err = inputs\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses_epoch.append(loss.item())\n",
    "    \n",
    "    losses_epoch_mean.append(np.mean(losses_epoch))\n",
    "    if epoch % int(NUM_EPOCHS/10) == 0:\n",
    "        print(f'Epoch {epoch}/{NUM_EPOCHS}, Loss: {np.mean(losses_epoch):.12f}')\n",
    "\n",
    "sns.lineplot(x=list(range(len(losses_epoch_mean))), y=losses_epoch_mean)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = []\n",
    "y_val_hat = []\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    with torch.no_grad():\n",
    "        y_val_hat_temp = model(inputs).round()\n",
    "    \n",
    "    y_val.extend(y_val_temp.numpy())\n",
    "    y_val_hat.extend(y_val_hat_temp.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_val, np.argmax(y_val_hat, axis=1))\n",
    "print(f'Accuracy: {acc*100:.2f} %')\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_val, np.argmax(y_val_hat, axis=1))\n",
    "sns.heatmap(cm, annot=True, xticklabels=chosen_labels, yticklabels=chosen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.feature_maps.size()\n",
    "\n",
    "# torch.save(model.state_dict(), 'test_CAM_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the test input and the validation class\n",
    "test_inp = val_loader.dataset[0][0]\n",
    "\n",
    "output = model(test_inp.unsqueeze(0))\n",
    "\n",
    "pred_class = torch.argmax(output, dim=1).item()\n",
    "print(pred_class)\n",
    "predicted_label = list(encoded_labels.keys())[list(encoded_labels.values()).index(pred_class)]\n",
    "\n",
    "plt.imshow(test_inp[0])\n",
    "plt.title(f'input spectrogram for -> {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inp = val_loader.dataset[1][0]\n",
    "output = model.forward(test_inp.unsqueeze(0))\n",
    "print(output)\n",
    "pred_class = torch.argmax(output, dim=1).item()\n",
    "predicted_label = list(encoded_labels.keys())[list(encoded_labels.values()).index(pred_class)]\n",
    "print(predicted_label)\n",
    "\n",
    "plt.imshow(test_inp[0])\n",
    "plt.title(f'input spectrogram for -> {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c1.shape)\n",
    "print(test_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = {}\n",
    "samples = {}\n",
    "for i, data in enumerate(val_loader):\n",
    "    inputs, y_val_temp = data\n",
    "    #print(inputs.shape, y_val_temp.shape)\n",
    "    for i in range(inputs.shape[0]):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the model output\n",
    "            output = model(inputs[i].unsqueeze(0))\n",
    "            # # Get the predicted class\n",
    "            _, pred_class = torch.max(output, 1)\n",
    "            predicted_label = list(encoded_labels.keys())[list(encoded_labels.values()).index(pred_class)]\n",
    "            # # Generate CAM for the first input and its predicted class\n",
    "            cam = model.getCAM(pred_class.item())\n",
    "            cam_vis = model.generate_cam_visualization(cam, inputs[0])\n",
    "\n",
    "            if predicted_label not in cams.keys():\n",
    "                cams[predicted_label] = [cam_vis]\n",
    "            else:\n",
    "                cams[predicted_label].append(cam_vis)\n",
    "\n",
    "            if predicted_label not in samples.keys():\n",
    "                samples[predicted_label] = inputs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc_block[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_cams = {}\n",
    "for key in cams.keys():\n",
    "    class_cams[key] = np.mean(cams[key], axis=0)\n",
    "\n",
    "for key in class_cams.keys():\n",
    "    plt.figure(figsize=(5, 10))\n",
    "    plt.imshow(class_cams[key], cmap='hot')\n",
    "    plt.title(f\"Class Activation Map for class: {key}\")\n",
    "    #plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in samples.keys():\n",
    "    plt.figure(figsize=(5, 10))\n",
    "    plt.imshow(samples[key][0], cmap='Greys')\n",
    "    plt.title(f\"Class Activation Map for class: {key}\")\n",
    "    #plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(samples.keys()), 2, figsize=(12, 2.5*len(samples.keys())))\n",
    "fig.suptitle('Samples and Corresponding Class Activation Maps')\n",
    "\n",
    "# Plot samples and CAMs side by side\n",
    "for idx, key in enumerate(samples.keys()):\n",
    "    # Left subplot - Sample\n",
    "    axs[idx, 0].imshow(samples[key][0], cmap='Greys')\n",
    "    axs[idx, 0].set_title(f'Sample ({key})')\n",
    "    \n",
    "    # Right subplot - CAM\n",
    "    im = axs[idx, 1].imshow(class_cams[key], cmap='hot')\n",
    "    axs[idx, 1].set_title(f'CAM ({key})')\n",
    "    \n",
    "    # Add colorbar to CAM subplot\n",
    "    fig.colorbar(im, ax=axs[idx, 1])\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in class_cams.keys():\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    ax.imshow(samples[key][0], cmap='Greys')\n",
    "    im = ax.imshow(class_cams[key], cmap='RdPu', alpha=0.35)\n",
    "    plt.title(f\"Class Activation Map for class: {key}\", pad=20)\n",
    "    plt.colorbar(im, label='Activation Strength', fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
