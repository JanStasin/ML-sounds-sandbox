{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jansta/miniconda3/envs/pytorch2/lib/python3.10/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jansta/learn/acoustics'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "#from plot_audio import plot_specgram, plot_waveform\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_ds_model import AudioDataset, AudioClassifNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mats = np.load('/Users/jansta/learn/acoustics/dict_mats.npy', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = dict_mats['A']['can_opening'][2]\n",
    "# print(t.shape)\n",
    "# t2 = t / t.max()\n",
    "# print(np.max(t), np.min(t), np.mean(t), np.std(t))\n",
    "# #plt.plot(t)\n",
    "# plt.hist(t, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dog', 'chirping_birds', 'vacuum_cleaner', 'thunderstorm', 'door_wood_knock', 'can_opening', 'crow', 'clapping', 'fireworks', 'chainsaw', 'airplane', 'mouse_click', 'pouring_water', 'train', 'sheep', 'water_drops', 'church_bells', 'clock_alarm', 'keyboard_typing', 'wind', 'footsteps', 'frog', 'cow', 'brushing_teeth', 'car_horn', 'crackling_fire', 'helicopter', 'drinking_sipping', 'rain', 'insects', 'laughing', 'hen', 'engine', 'breathing', 'crying_baby', 'hand_saw', 'coughing', 'glass_breaking', 'snoring', 'toilet_flush', 'pig', 'washing_machine', 'clock_tick', 'sneezing', 'rooster', 'sea_waves', 'siren', 'cat', 'door_wood_creaks', 'crickets'])\n"
     ]
    }
   ],
   "source": [
    "len(dict_mats['A']['can_opening'][3])\n",
    "\n",
    "all_labels = dict_mats['A'].keys()\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_labels = ['crickets', 'can_opening', 'dog','chirping_birds', 'chainsaw', 'crying_baby', 'rooster', 'sea_waves','toilet_flush'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosen_labels = list(all_labels)[16:26]\n",
    "encoded_labels = {}\n",
    "for i, label in enumerate(chosen_labels):\n",
    "    encoded_labels[label] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((64,431)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with transform\n",
    "dataset = AudioDataset(dict_mats['A'], chosen_labels, encoded_labels, transform=transform)\n",
    "datasetB = AudioDataset(dict_mats['B'], chosen_labels, encoded_labels, transform=transform)\n",
    "\n",
    "# # Split dataset\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(datasetB, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Input batch size: torch.Size([4, 1, 64, 431])\n",
      "Labels: tensor([7, 2, 6, 0])\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(f\"Input batch size: {inputs.size()}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(\"-\" * 30)\n",
    "    break  # Just to test the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class AudioClassifNet(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)  \n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)  \n",
    "#         self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         #self.fc1 = nn.Linear(64 * 4 * 53, 256)  # Adjusted based on pooling layers\n",
    "#         self.fc1 = nn.Linear(64 * 4 * 26, 256)  # Adjusted based on pooling layers\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.fc3 = nn.Linear(128, n_classes)  # classes\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)  # out: (BS, 16, 64, 431)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)   # out: (BS, 16, 32, 215)\n",
    "#         x = self.conv2(x)  # out: (BS, 32, 32, 215)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)   # out: (BS, 32, 16, 107)\n",
    "#         x = self.conv3(x)  # out: (BS, 64, 8, 107)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.pool(x)   # out: (BS, 64, 8, 53)\n",
    "#         x = self.pool(x)   # additional pooling layer - out: (BS, 64, 4, 26) \n",
    "#         x = self.flatten(x) # out: (4, 3328)\n",
    "#         x = self.fc1(x)  # out: (BS, 256)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)  # out: (BS, 128)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc3(x)  # out: (BS, n_classes)\n",
    "#         x = self.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_nans(tensor, name):\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaNs found in {name}\")\n",
    "        return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an  instance of the model:\n",
    "n_classes = len(chosen_labels)\n",
    "model = AudioClassifNet(n_classes)\n",
    "rate_l = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_func import run_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500, Loss: 2.5567397756708994\n",
      "Epoch 50/500, Loss: 0.0005706311221285\n",
      "Epoch 100/500, Loss: 0.0000090919989393\n",
      "Epoch 150/500, Loss: 0.0000006953815239\n",
      "Epoch 200/500, Loss: 0.0000000645716455\n",
      "Epoch 250/500, Loss: 0.0000000049670533\n",
      "Epoch 300/500, Loss: 0.0000000000000000\n",
      "Epoch 350/500, Loss: 0.0000000000000000\n",
      "Epoch 400/500, Loss: 0.0000000000000000\n",
      "Epoch 450/500, Loss: 0.0000000000000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.00105\u001b[39m][:\u001b[38;5;241m1\u001b[39m]: \n\u001b[0;32m----> 2\u001b[0m     losses_epoch_mean, acc, cm \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_l\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/learn/acoustics/training_func.py:47\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, train_loader, val_loader, rate_l, NUM_EPOCHS, save)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mint\u001b[39m(NUM_EPOCHS\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(losses_epoch)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.16f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(losses_epoch_mean))), y\u001b[38;5;241m=\u001b[39mlosses_epoch_mean)\n\u001b[1;32m     48\u001b[0m y_val \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m y_val_hat \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "for lr in [0.001, 0.00105][:1]: \n",
    "    losses_epoch_mean, acc, cm = run_training(model, train_loader, val_loader, rate_l=lr, NUM_EPOCHS = 500, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val = []\n",
    "# y_val_hat = []\n",
    "# for i, data in enumerate(val_loader):\n",
    "#     inputs, y_val_temp = data\n",
    "#     with torch.no_grad():\n",
    "#         y_val_hat_temp = model(inputs).round()\n",
    "    \n",
    "#     y_val.extend(y_val_temp.numpy())\n",
    "#     y_val_hat.extend(y_val_hat_temp.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Accuracy\n",
    "# acc = accuracy_score(y_val, np.argmax(y_val_hat, axis=1))\n",
    "# print(f'Accuracy: {acc*100:.2f} %')\n",
    "# # confusion matrix\n",
    "# cm = confusion_matrix(y_val, np.argmax(y_val_hat, axis=1))\n",
    "sns.heatmap(cm, annot=True, xticklabels=chosen_labels, yticklabels=chosen_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), f'audio_classification_model10_LR{rate_l}_a{acc*100:.0f}%.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
